#!/bin/bash
#SBATCH -J ab_train # Job name
#SBATCH --time=14-00:00:00 # Walltime
#SBATCH --cluster srf_gpu_01
#SBATCH --cpus-per-task=1
#SBATCH --mem=50000 # total memory (in MB) ### commented out
#SBATCH --gres=gpu:1
#SBATCH --ntasks=1 # 1 tasks
#SBATCH --overcommit
#SBATCH --nodes=1 # number of nodes
#SBATCH --partition=high-opig-gpu # Select a specific partition rather than default
#SBATCH -w nagagpu01.cpu.stats.ox.ac.uk # Provide a specific node/nodelist rather than the standard nodelist associated with the partition (useful if you have a data setup on one specific node)
#SBATCH --output=/homes/spoendli/slurm_%j.out # Writes standard output to this file. %j is jobnumber
#SBATCH --error=/homes/spoendli/slurm_%j.out # Writes error messages to this file. %j is jobnumber


source /data/localhost/not-backed-up/scratch/chinery/miniconda3/etc/profile.d/conda.sh
conda activate ab

cd /homes/spoendli/ABlooper/
git pull
python -u train_model.py > train_1804.out