{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ABDB import database as db\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from rich.progress import track\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation\n",
    "**1.1 Get data from SAbDab**\n",
    "\n",
    "Extract CDR sequencs and coordinate of backbone atoms from antibodies in SAbDab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to convert one letter, three letter and numerical amino acid codes\n",
    "aa1 = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa3 = [\"ALA\", \"CYS\", \"ASP\", \"GLU\", \"PHE\", \"GLY\", \"HIS\", \"ILE\", \"LYS\", \"LEU\", \"MET\", \"ASN\", \"PRO\", \"GLN\", \"ARG\", \"SER\",\n",
    "       \"THR\", \"VAL\", \"TRP\", \"TYR\", ]\n",
    "\n",
    "short2long = {}\n",
    "long2short = {}\n",
    "short2num = {}\n",
    "\n",
    "for ind in range(0, 20):\n",
    "    long2short[aa3[ind]] = aa1[ind]\n",
    "    short2long[aa1[ind]] = aa3[ind]\n",
    "    short2num[aa1[ind]] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to filter entries in SAbDAb\n",
    "\n",
    "def filter_abs(pdb_list):\n",
    "    '''\n",
    "    Filter a list of PDB ids obtained from SAbDab and removes FABS where one of the chains is missing or where\n",
    "    heavy and light chains have the same name.\n",
    "    '''\n",
    "    filtered_list = []\n",
    "    i = 0\n",
    "    \n",
    "    for pdb in track(pdb_list, description='Filter FABs'):\n",
    "        i += 1\n",
    "        fab = db.fetch(pdb).fabs[0]\n",
    "\n",
    "        if fab.VH == fab.VL:\n",
    "            continue\n",
    "        elif fab.VH == 'NA' or fab.VL == 'NA':\n",
    "            continue\n",
    "        else:\n",
    "            filtered_list.append(pdb)\n",
    "\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to extract and format relevant data from a SAbDab FAB. Given a FAB two dictionaries are returned for CDR and anchor \n",
    "# sequences and thier backbone coordinates\n",
    "\n",
    "def split_structure_in_regions(fab):\n",
    "    '''\n",
    "    Split FAB into regions.\n",
    "\n",
    "    Takes FAB as input an returns a dictionary with keys: regions, values: residues in region\n",
    "    regions = ['fwh1', 'cdrh1', 'fwh2', 'cdrh2', 'fwh3', 'cdrh3', 'fwh4', 'fwl1', 'cdrl1', 'fwl2', 'cdrl2', 'fwl3', 'cdrl3', 'fwl4']\n",
    "    '''\n",
    "    ab_regions = dict()\n",
    "    struc = fab.get_structure()\n",
    "\n",
    "    for chain in [fab.VH, fab.VL]:\n",
    "\n",
    "        # Chian.get_residues() is a generator that loops through residue\n",
    "        for residue in struc[chain].get_residues():\n",
    "\n",
    "            # residue.region indicates in which cdr or framework region the residue is\n",
    "            if residue.region in ab_regions:\n",
    "                ab_regions[residue.region].append(residue)\n",
    "            else:\n",
    "                ab_regions[residue.region] = [residue]\n",
    "\n",
    "    return ab_regions\n",
    "\n",
    "def get_slice(ab_regions, CDR):\n",
    "    '''\n",
    "    Returns a slice of residues containing a CDR plus two anchor residues on each side,\n",
    "    given a FAB split in to regions and a spefied CDR.\n",
    "    '''\n",
    "    chain = CDR[0].lower()\n",
    "    loop = CDR[1]\n",
    "\n",
    "    slice = ab_regions['fw' + chain + loop][-2:]\n",
    "    slice += ab_regions['cdr' + chain + loop]\n",
    "    slice += ab_regions['fw' + chain + str(int(loop) + 1 )][:2]\n",
    "\n",
    "    return slice\n",
    "\n",
    "def cdr_anchor_seq(ab_regions, CDR):\n",
    "    '''\n",
    "    Retruns sequence of a CDR plus two anchors on each side,\n",
    "    given a FAB split in to regions and a spefied CDR.\n",
    "    '''\n",
    "    slice = get_slice(ab_regions, CDR)\n",
    "    CDR_seq = []\n",
    "\n",
    "    for res in slice:\n",
    "        CDR_seq.append(long2short[res.resname])\n",
    "\n",
    "    return CDR_seq\n",
    "\n",
    "def cdr_anchor_BB_coord(ab_regions, CDR):\n",
    "    '''\n",
    "    Returns coordinates of backbone atoms of a CDR plus two anchors on each side,\n",
    "    given a FAB split in to regions and a spefied CDR.\n",
    "    '''\n",
    "    slice = get_slice(ab_regions, CDR)\n",
    "    CDR_BB_coord = np.zeros((len(slice), 4, 3))\n",
    "    BB_atoms = [\"CA\", \"C\", \"N\", \"CB\"]\n",
    "\n",
    "    for i in range(len(slice)):\n",
    "        res = slice[i]\n",
    "        for j in range(len(BB_atoms)):\n",
    "            atom = BB_atoms[j]\n",
    "\n",
    "            # if residue is glycine use CA coordinates for CB\n",
    "            if res.resname == 'GLY' and atom == 'CB':\n",
    "                atom = \"CA\"\n",
    "                \n",
    "            coord = res[atom].coord\n",
    "\n",
    "            CDR_BB_coord[i, j, :] = coord\n",
    "    \n",
    "    return CDR_BB_coord\n",
    "\n",
    "def get_cdr_anchor_seqs(ab_regions, CDRs = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]):\n",
    "    '''\n",
    "    Get sequences of all CDRs in a FAB.\n",
    "\n",
    "    Returns a dictionary with keys: CDRs, values: CDR + anchor sequence\n",
    "    '''\n",
    "    CDR_seqs = dict()\n",
    "\n",
    "    for CDR in CDRs:\n",
    "        CDR_seqs[CDR] = cdr_anchor_seq(ab_regions, CDR)\n",
    "   \n",
    "    return CDR_seqs\n",
    "\n",
    "def get_cdr_anchor_BB_coords(cdr_residues, CDRs = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]):\n",
    "    '''\n",
    "    Get backbone coordinates of all CDRs in a FAB.\n",
    "    \n",
    "    Returns a dictionary with keys: CDRs, values: CDR + anchor backbone coordinates\n",
    "    '''\n",
    "    CDR_BB_coords = dict()\n",
    "\n",
    "    for CDR in CDRs:\n",
    "        CDR_BB_coords[CDR] = cdr_anchor_BB_coord(cdr_residues, CDR)\n",
    "   \n",
    "    return CDR_BB_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve all PDB ids from SAbDab and runs functions in above cells for each individual FAB. \n",
    "def get_sabdab_fabs():\n",
    "    '''\n",
    "    Get all fabs from sabdab, extracts CDR sequences and coordinates and formats the data for the next steps.\n",
    "\n",
    "    returns CDR_seqs: list of dictionaries,\n",
    "                      each dictionary contains data of one FAB, keys: CDR, value: CDR sequence\n",
    "    returns CDR_BB_coords: list of dictionaries,\n",
    "                           each dictionary contains data of one FAB, keys: CDR, value: CDR backbone coordinates\n",
    "    '''\n",
    "    # use imgt numbering\n",
    "    db.set_numbering_scheme(\"imgt\")\n",
    "    db.set_region_definition(\"imgt\")\n",
    "\n",
    "    # list of all pdb ids in SAbDab\n",
    "    all_pdbs_in_sabdab = list(db.db_summary.keys())\n",
    "    all_pdbs_in_sabdab = filter_abs(all_pdbs_in_sabdab)\n",
    "\n",
    "    CDR_seqs = list()\n",
    "    CDR_BB_coords = list()\n",
    "\n",
    "    for pdb_id in track(all_pdbs_in_sabdab, description='Load data from SAbDab'):\n",
    "        pdb = db.fetch(pdb_id)\n",
    "        for fab in pdb.fabs:\n",
    "            try: # some fab have errors and throw exceptions, ignore these\n",
    "                ab_regions = split_structure_in_regions(fab)\n",
    "                cdr_seqs = get_cdr_anchor_seqs(ab_regions)\n",
    "                cdr_BB_coords = get_cdr_anchor_BB_coords(ab_regions)\n",
    "\n",
    "                CDR_seqs.append(cdr_seqs)\n",
    "                CDR_BB_coords.append(cdr_BB_coords)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return CDR_seqs, CDR_BB_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code that runs the above functions to download all FABs from SAbDab and extract data about CDR and anchor sequences and backbone coordinates. Data is saved in a json as the code interacting with SAbDAb is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Load data from SAbDab <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:08</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Load data from SAbDab \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:08\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CDR_seqs, CDR_BB_coords = get_sabdab_fabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data/CDR_BB_coords.npy', 'wb') as outfile:\n",
    "    np.save(outfile, CDR_BB_coords)\n",
    "\n",
    "with open('train_data/CDR_seqs.npy', 'wb') as outfile:\n",
    "    np.save(outfile, CDR_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data/CDR_BB_coords.npy', 'rb') as infile:\n",
    "    a = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('train_data/CDR_seqs.npy', 'rb') as infile:\n",
    "    b = np.load(infile, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Format data to model inputs**\n",
    "\n",
    "Data loaded from SAbDab is reformated to model inputs and the training outputs. \n",
    "\n",
    "Each backbone atom corresponds to one node in the graph. The atoms are encoded into a vector with 41 elements (one-hot encoding of amino acid residue, one-hot encoding of atom type, one-hot encoding of CDR loop, positional encoding within loop).\n",
    "\n",
    "The input coordinates of each backbone atoms are processed as follows. Anchor residues keep their original position, the CDR residues are spaced equally on a straigt line between the two anchors.\n",
    "\n",
    "The training output coordinates correspond to the backbone coordinates from the crystal structure formated identically to the input coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that convert data extracted from SAbDab to model input\n",
    "\n",
    "def encode(x, classes):\n",
    "    '''\n",
    "    One hot encodes a scalar x into a vector of length classes.\n",
    "    This is the function used for Sequence encoding.\n",
    "    '''\n",
    "    one_hot = np.zeros(classes)\n",
    "    one_hot[x] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def one_hot(num_list, classes=20):\n",
    "    '''\n",
    "    One hot encodes a 1D vector x.\n",
    "    This is the function used for Sequence encoding.\n",
    "    '''\n",
    "    end_shape = (len(num_list), classes)\n",
    "    finish = np.zeros(end_shape)\n",
    "    for i in range(end_shape[0]):\n",
    "        finish[i] = encode(num_list[i], classes)\n",
    "\n",
    "    return finish\n",
    "\n",
    "def which_loop(loop_seq, cdr):\n",
    "    '''\n",
    "    Adds a one-hot encoded vector to each node describing which CDR it belongs to.\n",
    "    '''\n",
    "    CDRs = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\", \"Anchor\"]\n",
    "    loop = np.zeros((len(loop_seq), len(CDRs)))\n",
    "    loop[:, -1] = 1\n",
    "    loop[2:-2] = np.array([1.0 if cdr == x else 0.0 for x in (CDRs)])[None].repeat(len(loop_seq) - 4, axis=0)\n",
    "\n",
    "    return loop\n",
    "\n",
    "def positional_encoding(sequence, n=5):\n",
    "    '''\n",
    "    Gives the network information on how close each resdiue is to the anchors\n",
    "    '''\n",
    "    encs = []\n",
    "    L = len(sequence)\n",
    "    for i in range(n):\n",
    "        encs.append(np.cos((2 ** i) * np.pi * np.arange(L) / L))\n",
    "        encs.append(np.sin((2 ** i) * np.pi * np.arange(L) / L))\n",
    "\n",
    "    return np.array(encs).transpose()\n",
    "\n",
    "def res_to_atom(res_encoding, n_atoms=4):\n",
    "    '''\n",
    "    Adds a one-hot encoded vector to each node describing what atom type it is.\n",
    "    '''\n",
    "    out_shape = (res_encoding.shape[0], n_atoms, 41)\n",
    "    atom_encoding = np.zeros(out_shape)\n",
    "\n",
    "    for i in range(len(res_encoding)):\n",
    "        for j in range(n_atoms):\n",
    "            atom_encoding[i, j, 0:37] = res_encoding[i]\n",
    "            # add one-hot encoding for atom type\n",
    "            atom_encoding[i, j, 37:] = one_hot([j], classes=n_atoms) \n",
    "\n",
    "    return atom_encoding\n",
    "\n",
    "def prepare_input_loop(CDR_coord, CDR_seq, CDR):\n",
    "    '''\n",
    "    Generates input features to be fed into the network for a single CDR\n",
    "    '''\n",
    "    CDR_input_coord = copy.deepcopy(CDR_coord)\n",
    "    # put CDR residues equally spaced on straight line between anchor residues \n",
    "    CDR_input_coord[1:-1] = np.linspace(CDR_coord[1], CDR_coord[-2], len(CDR_coord) - 2)\n",
    "    # CDR_input_coord = rearrange(torch.tensor(CDR_input_coords), \"i a d -> () (i a) d\").float()\n",
    "\n",
    "    one_hot_encoding = one_hot(np.array([short2num[amino] for amino in CDR_seq]))\n",
    "    loop = which_loop(CDR_seq, CDR)\n",
    "    positional = positional_encoding(CDR_seq)\n",
    "    res_encoding = np.concatenate([one_hot_encoding, positional, loop], axis=1)\n",
    "    atom_encoding = res_to_atom(res_encoding)\n",
    "\n",
    "    # encoding = res_to_atom(torch.tensor(np.concatenate([one_hot_encoding, positional, loop], axis=1)).float())\n",
    "    # encoding = rearrange(encoding, \"i a d -> () (i a) d\")\n",
    "\n",
    "    return CDR_input_coord, atom_encoding\n",
    "\n",
    "def prepare_model_input(CDR_seq, CDR_BB_coord):\n",
    "    '''\n",
    "    Prepares model inputs for a single FAB\n",
    "    '''\n",
    "    encodings = []\n",
    "    geomins = []\n",
    "    \n",
    "    for CDR in CDR_BB_coord:\n",
    "        geom, encode = prepare_input_loop(CDR_BB_coord[CDR], CDR_seq[CDR], CDR)\n",
    "        encodings.append(encode)\n",
    "        geomins.append(geom)\n",
    "\n",
    "    # concatenate encodings and geoms into single array\n",
    "    encodings = np.concatenate(encodings, axis=0)\n",
    "    geomins = np.concatenate(geomins, axis=0)\n",
    "    # format to tensor\n",
    "    encodings = torch.from_numpy(encodings)\n",
    "    geomins = torch.from_numpy(geomins)\n",
    "    # rearrange tensors that atoms in one residue are nolonger grouped\n",
    "    encodings = rearrange(encodings, \"i a d -> () (i a) d\")\n",
    "    geomins = rearrange(geomins, \"i a d -> () (i a) d\")\n",
    "\n",
    "    return geomins, encodings\n",
    "\n",
    "def prepare_model_inputs(CDR_seqs, CDR_BB_coords):\n",
    "    '''\n",
    "    Prepares model inputs for a list of FABs\n",
    "    '''\n",
    "    encodings = []\n",
    "    geomins = []\n",
    "\n",
    "    for i in track(range(len(CDR_seqs)), description='Preparing model inputs'):\n",
    "        geom, encode = prepare_model_input(CDR_seqs[i], CDR_BB_coords[i])\n",
    "        encodings.append(encode)\n",
    "        geomins.append(geom)\n",
    "\n",
    "    return geomins, encodings\n",
    "\n",
    "def prepare_model_output(CDR_BB_coords):\n",
    "    '''\n",
    "    Prepares model outputs for training, formated identically to inputs\n",
    "    '''\n",
    "    geomouts = []\n",
    "    for CDR_BB_coord in track(CDR_BB_coords, description='Preparing model outputs'):\n",
    "        geomout = []\n",
    "        for _, coords in CDR_BB_coord.items():\n",
    "            geomout.append(coords)\n",
    "\n",
    "        # concatenate geoms into single array\n",
    "        geomout = np.concatenate(geomout, axis=0)\n",
    "        # format to tensor\n",
    "        geomout = torch.from_numpy(geomout)\n",
    "        # rearrange tensor\n",
    "        geomout = rearrange(geomout, \"i a d -> () (i a) d\")\n",
    "\n",
    "        geomouts.append(geomout)\n",
    "    return geomouts\n",
    "\n",
    "def concatenate_data(encodings, geomins, geomouts):\n",
    "    '''\n",
    "    Puts encodings, geomins and geomouts into a single array.\n",
    "    '''\n",
    "    data = []\n",
    "    for i in range(len(encodings)):\n",
    "        # potentially change list to dict\n",
    "        data.append({'encodings': encodings[i],\n",
    "                     'geomins': geomins[i],\n",
    "                     'geomouts': geomouts[i]})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a81517657d4e9b9b146af7f5cda1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14c6ae2e656412492afc7de6879d4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geomins, node_encodings = prepare_model_inputs(CDR_seqs, CDR_BB_coords)\n",
    "geomouts = prepare_model_output(CDR_BB_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = concatenate_data(node_encodings, geomins, geomouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Prepare data for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4869, 1624, 1624)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data needs to be a single array or list containing encodings, geomins and geomouts\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train, validation = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "len(train), len(validation), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train, \n",
    "                                               batch_size=1,    # Batch size\n",
    "                                               num_workers=1,   # Number of cpu's allocated to load the data (recommended is 4/GPU)\n",
    "                                               shuffle=True,    # Whether to randomly shuffle data\n",
    "                                               pin_memory=True, # Enables faster data transfer to CUDA-enabled GPUs (page-locked memory)\n",
    "                                               )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test, \n",
    "                                              batch_size=1,    # Batch size\n",
    "                                              num_workers=1,   # Number of cpu's allocated to load the data (recommended is 4/GPU)\n",
    "                                              shuffle=True,    # Whether to randomly shuffle data\n",
    "                                              pin_memory=True, # Enables faster data transfer to CUDA-enabled GPUs (page-locked memory)\n",
    "                                              )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(validation, \n",
    "                                             batch_size=1,    # Batch size\n",
    "                                             num_workers=1,   # Number of cpu's allocated to load the data (recommended is 4/GPU)\n",
    "                                             shuffle=True,    # Whether to randomly shuffle data\n",
    "                                             pin_memory=True, # Enables faster data transfer to CUDA-enabled GPUs (page-locked memory)\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and test data sets\n",
    "Input data are two arrays. H: one-hot encoded features of the atom. Array has size len(atoms) x 41. X: coordinates of backbone atoms, CDR residues are placed with equal spacing on a straight line between the two anchors. Array has size len(atoms) x 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCDRDataset(Dataset):\n",
    "    def __init__(self, geom_ins_path, aminos_path, geom_out_path):\n",
    "\n",
    "        with open(geom_ins_path, 'r') as f:\n",
    "            self.geom_ins = json.load(f)\n",
    "        with open(aminos_path, 'r') as f:\n",
    "            self.aminos = json.load(f)\n",
    "        with open(geom_out_path, 'r') as f:\n",
    "            self.geom_out = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.aminos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        geom_in = self.geom_ins[idx]\n",
    "        aminos = self.aminos[idx]\n",
    "        geom_out = self.geom_out[idx]\n",
    "        return geom_in, aminos, geom_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomCDRDataset(...)\n",
    "test_data = CustomCDRDataset(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the EGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedEGNN(torch.nn.Module):\n",
    "    def __init__(self, node_dim, message_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        edge_input_dim = (node_dim * 2) + 1\n",
    "\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(edge_input_dim, 2*edge_input_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(2*edge_input_dim, message_dim),\n",
    "            torch.nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(node_dim + message_dim, 2*node_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(2*node_dim, node_dim),\n",
    "        )\n",
    "\n",
    "        self.coors_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(message_dim, 2*message_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(2*message_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, node_features, coordinates, mask):                                                        # We pass in a mask that tells us what nodes to consider and which to ignore.\n",
    "        pair_mask = rearrange(mask, 'b j -> b () j ()') * rearrange(mask, 'b i -> b i () ()')                   # We calculate a mask for which interactions to consider.\n",
    "\n",
    "        rel_coors = rearrange(coordinates, 'b i d -> b i () d') - rearrange(coordinates, 'b j d -> b () j d')  \n",
    "        rel_dist = (rel_coors ** 2).sum(dim=-1, keepdim=True)                                                  \n",
    "\n",
    "        feats_j = rearrange(node_features, 'b j d -> b () j d')      \n",
    "        feats_i = rearrange(node_features, 'b i d -> b i () d')\n",
    "        feats_i, feats_j = torch.broadcast_tensors(feats_i, feats_j)\n",
    "\n",
    "        edge_input = torch.cat((feats_i, feats_j, rel_dist), dim=-1)\n",
    "\n",
    "        m_ij = self.edge_mlp(edge_input)\n",
    "\n",
    "        coor_weights = pair_mask * self.coors_mlp(m_ij)                                                          # We multiply the predicted weight by the mask (masked residue pairs will have zero weight).\n",
    "        coor_weights = rearrange(coor_weights, 'b i j () -> b i j')\n",
    "\n",
    "        rel_coors_normed = rel_coors / rel_dist.clip(min = 1e-8)    \n",
    "\n",
    "        coors_out = coordinates + torch.einsum('b i j, b i j c -> b i c', coor_weights, rel_coors_normed)  \n",
    "\n",
    "        m_i = torch.einsum('b i d, b -> b i d', (pair_mask * m_ij).sum(dim=-2), 1/mask.sum(-1))                  # To average we divide over the length for each batch (length = sum(mask)).\n",
    "\n",
    "        node_mlp_input = torch.cat((node_features, m_i), dim=-1)\n",
    "        node_out = node_features + mask.unsqueeze(-1) * self.node_mlp(node_mlp_input)                            # We set the update for maked residues to zero. \n",
    "\n",
    "        return node_out, coors_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNNModel(torch.nn.Module):\n",
    "    def __init__(self, node_dim, layers=4, message_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([MaskedEGNN(node_dim, message_dim = message_dim) for _ in range(layers)])   # Initialise as many EGNN layers as needed\n",
    "\n",
    "    def forward(self, node_features, coordinates, mask):\n",
    "\n",
    "        for layer in self.layers:                                                                            \n",
    "            node_features, coordinates = layer(node_features, coordinates, mask)                                      # Update node features and coordinates for each layer in the model\n",
    "        \n",
    "        return node_features, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "# this should be rmsd\n",
    "loss_fn = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (geom_ins, aminos, geom_out) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(aminos, geom_ins)\n",
    "        loss = loss_fn(pred, geom_out)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for geom_ins, aminos, geom_out in dataloader:\n",
    "            pred = model(aminos, geom_ins)\n",
    "            test_loss += loss_fn(pred, geom_out).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b68f9eedec4825490a01d6907fa8139c5284fcb4395b458f0f41fe7d228fceae"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('ab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
