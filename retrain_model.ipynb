{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabian/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ABDB import database as db\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from rich.progress import track\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation\n",
    "**1.1 Get data from SAbDab**\n",
    "\n",
    "Extract CDR sequencs and coordinate of backbone atoms from antibodies in SAbDab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to convert one letter, three letter and numerical amino acid codes\n",
    "aa1 = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa3 = [\"ALA\", \"CYS\", \"ASP\", \"GLU\", \"PHE\", \"GLY\", \"HIS\", \"ILE\", \"LYS\", \"LEU\", \"MET\", \"ASN\", \"PRO\", \"GLN\", \"ARG\", \"SER\",\n",
    "       \"THR\", \"VAL\", \"TRP\", \"TYR\", ]\n",
    "\n",
    "short2long = {}\n",
    "long2short = {}\n",
    "short2num = {}\n",
    "\n",
    "for ind in range(0, 20):\n",
    "    long2short[aa3[ind]] = aa1[ind]\n",
    "    short2long[aa1[ind]] = aa3[ind]\n",
    "    short2num[aa1[ind]] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to filter entries in SAbDAb\n",
    "\n",
    "def filter_abs(pdb_list):\n",
    "    '''\n",
    "    Filter a list of PDB ids obtained from SAbDab and removes FABS where one of the chains is missing or where\n",
    "    heavy and light chains have the same name.\n",
    "    '''\n",
    "    filtered_list = []\n",
    "    i = 0\n",
    "    \n",
    "    for pdb in track(pdb_list, description='Filter FABs'):\n",
    "        i += 1\n",
    "        fab = db.fetch(pdb).fabs[0]\n",
    "\n",
    "        if fab.VH == fab.VL:\n",
    "            continue\n",
    "        elif fab.VH == 'NA' or fab.VL == 'NA':\n",
    "            continue\n",
    "        else:\n",
    "            filtered_list.append(pdb)\n",
    "\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to extract and format relevant data from a SAbDab FAB. Given a FAB two dictionaries are returned for CDR and anchor \n",
    "# sequences and thier backbone coordinates\n",
    "\n",
    "def split_structure_in_regions(fab):\n",
    "    '''\n",
    "    Split FAB into regions.\n",
    "\n",
    "    Takes FAB as input an returns a dictionary with keys: regions, values: residues in region\n",
    "    regions = ['fwh1', 'cdrh1', 'fwh2', 'cdrh2', 'fwh3', 'cdrh3', 'fwh4', 'fwl1', 'cdrl1', 'fwl2', 'cdrl2', 'fwl3', 'cdrl3', 'fwl4']\n",
    "    '''\n",
    "    ab_regions = dict()\n",
    "    struc = fab.get_structure()\n",
    "\n",
    "    for chain in [fab.VH, fab.VL]:\n",
    "\n",
    "        # Chian.get_residues() is a generator that loops through residue\n",
    "        for residue in struc[chain].get_residues():\n",
    "\n",
    "            # residue.region indicates in which cdr or framework region the residue is\n",
    "            if residue.region in ab_regions:\n",
    "                ab_regions[residue.region].append(residue)\n",
    "            else:\n",
    "                ab_regions[residue.region] = [residue]\n",
    "\n",
    "    return ab_regions\n",
    "\n",
    "def get_slice(ab_regions, CDR):\n",
    "    '''\n",
    "    Returns a slice of residues containing a CDR plus two anchor residues on each side,\n",
    "    given a FAB split in to regions and a spefied CDR.\n",
    "    '''\n",
    "    chain = CDR[0].lower()\n",
    "    loop = CDR[1]\n",
    "\n",
    "    slice = ab_regions['fw' + chain + loop][-2:]\n",
    "    slice += ab_regions['cdr' + chain + loop]\n",
    "    slice += ab_regions['fw' + chain + str(int(loop) + 1 )][:2]\n",
    "\n",
    "    return slice\n",
    "\n",
    "def cdr_anchor_seq(ab_regions, CDR):\n",
    "    '''\n",
    "    Retruns sequence of a CDR plus two anchors on each side,\n",
    "    given a FAB split in to regions and a spefied CDR.\n",
    "    '''\n",
    "    slice = get_slice(ab_regions, CDR)\n",
    "    CDR_seq = []\n",
    "\n",
    "    for res in slice:\n",
    "        CDR_seq.append(long2short[res.resname])\n",
    "\n",
    "    return CDR_seq\n",
    "\n",
    "def cdr_anchor_BB_coord(ab_regions, CDR):\n",
    "    '''\n",
    "    Returns coordinates of backbone atoms of a CDR plus two anchors on each side,\n",
    "    given a FAB split in to regions and a spefied CDR.\n",
    "    '''\n",
    "    slice = get_slice(ab_regions, CDR)\n",
    "    CDR_BB_coord = np.zeros((len(slice), 4, 3))\n",
    "    BB_atoms = [\"CA\", \"C\", \"N\", \"CB\"]\n",
    "\n",
    "    for i in range(len(slice)):\n",
    "        res = slice[i]\n",
    "        for j in range(len(BB_atoms)):\n",
    "            atom = BB_atoms[j]\n",
    "\n",
    "            # if residue is glycine use CA coordinates for CB\n",
    "            if res.resname == 'GLY' and atom == 'CB':\n",
    "                atom = \"CA\"\n",
    "                \n",
    "            coord = res[atom].coord\n",
    "\n",
    "            CDR_BB_coord[i, j, :] = coord\n",
    "    \n",
    "    return CDR_BB_coord\n",
    "\n",
    "def get_cdr_anchor_seqs(ab_regions, CDRs = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]):\n",
    "    '''\n",
    "    Get sequences of all CDRs in a FAB.\n",
    "\n",
    "    Returns a dictionary with keys: CDRs, values: CDR + anchor sequence\n",
    "    '''\n",
    "    CDR_seqs = dict()\n",
    "\n",
    "    for CDR in CDRs:\n",
    "        CDR_seqs[CDR] = cdr_anchor_seq(ab_regions, CDR)\n",
    "   \n",
    "    return CDR_seqs\n",
    "\n",
    "def get_cdr_anchor_BB_coords(cdr_residues, CDRs = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]):\n",
    "    '''\n",
    "    Get backbone coordinates of all CDRs in a FAB.\n",
    "    \n",
    "    Returns a dictionary with keys: CDRs, values: CDR + anchor backbone coordinates\n",
    "    '''\n",
    "    CDR_BB_coords = dict()\n",
    "\n",
    "    for CDR in CDRs:\n",
    "        CDR_BB_coords[CDR] = cdr_anchor_BB_coord(cdr_residues, CDR)\n",
    "   \n",
    "    return CDR_BB_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve all PDB ids from SAbDab and runs functions in above cells for each individual FAB. \n",
    "def get_sabdab_fabs():\n",
    "    '''\n",
    "    Get all fabs from sabdab, extracts CDR sequences and coordinates and formats the data for the next steps.\n",
    "\n",
    "    returns CDR_seqs: list of dictionaries,\n",
    "                      each dictionary contains data of one FAB, keys: CDR, value: CDR sequence\n",
    "    returns CDR_BB_coords: list of dictionaries,\n",
    "                           each dictionary contains data of one FAB, keys: CDR, value: CDR backbone coordinates\n",
    "    '''\n",
    "    # use imgt numbering\n",
    "    db.set_numbering_scheme(\"imgt\")\n",
    "    db.set_region_definition(\"imgt\")\n",
    "\n",
    "    # list of all pdb ids in SAbDab\n",
    "    all_pdbs_in_sabdab = list(db.db_summary.keys())\n",
    "    all_pdbs_in_sabdab = filter_abs(all_pdbs_in_sabdab)\n",
    "\n",
    "    CDR_seqs = list()\n",
    "    CDR_BB_coords = list()\n",
    "\n",
    "    for pdb_id in track(all_pdbs_in_sabdab, description='Load data from SAbDab'):\n",
    "        pdb = db.fetch(pdb_id)\n",
    "        for fab in pdb.fabs:\n",
    "            try: # some fab have errors and throw exceptions, ignore these\n",
    "                ab_regions = split_structure_in_regions(fab)\n",
    "                cdr_seqs = get_cdr_anchor_seqs(ab_regions)\n",
    "                cdr_BB_coords = get_cdr_anchor_BB_coords(ab_regions)\n",
    "\n",
    "                CDR_seqs.append(cdr_seqs)\n",
    "                CDR_BB_coords.append(cdr_BB_coords)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return CDR_seqs, CDR_BB_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code that runs the above functions to download all FABs from SAbDab and extract data about CDR and anchor sequences and backbone coordinates. Data is saved in a json as the code interacting with SAbDAb is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Load data from SAbDab <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:08</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Load data from SAbDab \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:08\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CDR_seqs, CDR_BB_coords = get_sabdab_fabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data/CDR_BB_coords.npy', 'wb') as outfile:\n",
    "    np.save(outfile, CDR_BB_coords)\n",
    "\n",
    "with open('train_data/CDR_seqs.npy', 'wb') as outfile:\n",
    "    np.save(outfile, CDR_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data/CDR_BB_coords.npy', 'rb') as infile:\n",
    "    CDR_BB_coords = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('train_data/CDR_seqs.npy', 'rb') as infile:\n",
    "    CDR_seqs = np.load(infile, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Format data to model inputs**\n",
    "\n",
    "Data loaded from SAbDab is reformated to model inputs and the training outputs. \n",
    "\n",
    "Each backbone atom corresponds to one node in the graph. The atoms are encoded into a vector with 41 elements (one-hot encoding of amino acid residue, one-hot encoding of atom type, one-hot encoding of CDR loop, positional encoding within loop).\n",
    "\n",
    "The input coordinates of each backbone atoms are processed as follows. Anchor residues keep their original position, the CDR residues are spaced equally on a straigt line between the two anchors.\n",
    "\n",
    "The training output coordinates correspond to the backbone coordinates from the crystal structure formated identically to the input coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that convert data extracted from SAbDab to model input\n",
    "\n",
    "def encode(x, classes):\n",
    "    '''\n",
    "    One hot encodes a scalar x into a vector of length classes.\n",
    "    This is the function used for Sequence encoding.\n",
    "    '''\n",
    "    one_hot = np.zeros(classes)\n",
    "    one_hot[x] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def one_hot(num_list, classes=20):\n",
    "    '''\n",
    "    One hot encodes a 1D vector x.\n",
    "    This is the function used for Sequence encoding.\n",
    "    '''\n",
    "    end_shape = (len(num_list), classes)\n",
    "    finish = np.zeros(end_shape)\n",
    "    for i in range(end_shape[0]):\n",
    "        finish[i] = encode(num_list[i], classes)\n",
    "\n",
    "    return finish\n",
    "\n",
    "def which_loop(loop_seq, cdr):\n",
    "    '''\n",
    "    Adds a one-hot encoded vector to each node describing which CDR it belongs to.\n",
    "    '''\n",
    "    CDRs = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\", \"Anchor\"]\n",
    "    loop = np.zeros((len(loop_seq), len(CDRs)))\n",
    "    loop[:, -1] = 1\n",
    "    loop[2:-2] = np.array([1.0 if cdr == x else 0.0 for x in (CDRs)])[None].repeat(len(loop_seq) - 4, axis=0)\n",
    "\n",
    "    return loop\n",
    "\n",
    "def positional_encoding(sequence, n=5):\n",
    "    '''\n",
    "    Gives the network information on how close each resdiue is to the anchors\n",
    "    '''\n",
    "    encs = []\n",
    "    L = len(sequence)\n",
    "    for i in range(n):\n",
    "        encs.append(np.cos((2 ** i) * np.pi * np.arange(L) / L))\n",
    "        encs.append(np.sin((2 ** i) * np.pi * np.arange(L) / L))\n",
    "\n",
    "    return np.array(encs).transpose()\n",
    "\n",
    "def res_to_atom(res_encoding, n_atoms=4):\n",
    "    '''\n",
    "    Adds a one-hot encoded vector to each node describing what atom type it is.\n",
    "    '''\n",
    "    out_shape = (res_encoding.shape[0], n_atoms, 41)\n",
    "    atom_encoding = np.zeros(out_shape)\n",
    "\n",
    "    for i in range(len(res_encoding)):\n",
    "        for j in range(n_atoms):\n",
    "            atom_encoding[i, j, 0:37] = res_encoding[i]\n",
    "            # add one-hot encoding for atom type\n",
    "            atom_encoding[i, j, 37:] = one_hot([j], classes=n_atoms) \n",
    "\n",
    "    return atom_encoding\n",
    "\n",
    "def prepare_input_loop(CDR_coord, CDR_seq, CDR):\n",
    "    '''\n",
    "    Generates input features to be fed into the network for a single CDR\n",
    "    '''\n",
    "    CDR_input_coord = copy.deepcopy(CDR_coord)\n",
    "    # put CDR residues equally spaced on straight line between anchor residues \n",
    "    CDR_input_coord[1:-1] = np.linspace(CDR_coord[1], CDR_coord[-2], len(CDR_coord) - 2)\n",
    "    # CDR_input_coord = rearrange(torch.tensor(CDR_input_coords), \"i a d -> () (i a) d\").float()\n",
    "\n",
    "    one_hot_encoding = one_hot(np.array([short2num[amino] for amino in CDR_seq]))\n",
    "    loop = which_loop(CDR_seq, CDR)\n",
    "    positional = positional_encoding(CDR_seq)\n",
    "    res_encoding = np.concatenate([one_hot_encoding, positional, loop], axis=1)\n",
    "    atom_encoding = res_to_atom(res_encoding)\n",
    "\n",
    "    # encoding = res_to_atom(torch.tensor(np.concatenate([one_hot_encoding, positional, loop], axis=1)).float())\n",
    "    # encoding = rearrange(encoding, \"i a d -> () (i a) d\")\n",
    "\n",
    "    return CDR_input_coord, atom_encoding\n",
    "\n",
    "def prepare_model_input(CDR_seq, CDR_BB_coord):\n",
    "    '''\n",
    "    Prepares model inputs for a single FAB\n",
    "    '''\n",
    "    encodings = []\n",
    "    geomins = []\n",
    "    \n",
    "    for CDR in CDR_BB_coord:\n",
    "        geom, encode = prepare_input_loop(CDR_BB_coord[CDR], CDR_seq[CDR], CDR)\n",
    "        encodings.append(encode)\n",
    "        geomins.append(geom)\n",
    "\n",
    "    # concatenate encodings and geoms into single array\n",
    "    encodings = np.concatenate(encodings, axis=0)\n",
    "    geomins = np.concatenate(geomins, axis=0)\n",
    "    # format to tensor\n",
    "    encodings = torch.from_numpy(encodings)\n",
    "    geomins = torch.from_numpy(geomins)\n",
    "    # rearrange tensors that atoms in one residue are nolonger grouped\n",
    "    encodings = rearrange(encodings, \"i a d -> (i a) d\")\n",
    "    geomins = rearrange(geomins, \"i a d -> (i a) d\")\n",
    "\n",
    "    return geomins, encodings\n",
    "\n",
    "def prepare_model_inputs(CDR_seqs, CDR_BB_coords):\n",
    "    '''\n",
    "    Prepares model inputs for a list of FABs\n",
    "    '''\n",
    "    encodings = []\n",
    "    geomins = []\n",
    "\n",
    "    for i in track(range(len(CDR_seqs)), description='Preparing model inputs'):\n",
    "        geom, encode = prepare_model_input(CDR_seqs[i], CDR_BB_coords[i])\n",
    "        encodings.append(encode)\n",
    "        geomins.append(geom)\n",
    "\n",
    "    return geomins, encodings\n",
    "\n",
    "def prepare_model_output(CDR_BB_coords):\n",
    "    '''\n",
    "    Prepares model outputs for training, formated identically to inputs\n",
    "    '''\n",
    "    geomouts = []\n",
    "    for CDR_BB_coord in track(CDR_BB_coords, description='Preparing model outputs'):\n",
    "        geomout = []\n",
    "        for _, coords in CDR_BB_coord.items():\n",
    "            geomout.append(coords)\n",
    "\n",
    "        # concatenate geoms into single array\n",
    "        geomout = np.concatenate(geomout, axis=0)\n",
    "        # format to tensor\n",
    "        geomout = torch.from_numpy(geomout)\n",
    "        # rearrange tensor\n",
    "        geomout = rearrange(geomout, \"i a d -> (i a) d\")\n",
    "\n",
    "        geomouts.append(geomout)\n",
    "    return geomouts\n",
    "\n",
    "def concatenate_data(encodings, geomins, geomouts):\n",
    "    '''\n",
    "    Puts encodings, geomins and geomouts into a single array.\n",
    "    '''\n",
    "    data = []\n",
    "    for i in range(len(encodings)):\n",
    "        # potentially change list to dict\n",
    "        data.append({'encodings': encodings[i],\n",
    "                     'geomins': geomins[i],\n",
    "                     'geomouts': geomouts[i]})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Preparing model outputs <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Preparing model outputs \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geomins, node_encodings = prepare_model_inputs(CDR_seqs, CDR_BB_coords)\n",
    "geomouts = prepare_model_output(CDR_BB_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8117"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = concatenate_data(node_encodings, geomins, geomouts)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Prepare data for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4869, 1624, 1624)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data needs to be a single array or list containing encodings, geomins and geomouts\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train, validation = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "len(train), len(validation), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train, \n",
    "                                               batch_size=1,    # Batch size\n",
    "                                               num_workers=1,   # Number of cpu's allocated to load the data (recommended is 4/GPU)\n",
    "                                               shuffle=True,    # Whether to randomly shuffle data\n",
    "                                               pin_memory=True, # Enables faster data transfer to CUDA-enabled GPUs (page-locked memory)\n",
    "                                               )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test, \n",
    "                                              batch_size=1,\n",
    "                                              num_workers=1,\n",
    "                                              shuffle=True,\n",
    "                                              pin_memory=True,\n",
    "                                              )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(validation, \n",
    "                                             batch_size=1,\n",
    "                                             num_workers=1,\n",
    "                                             shuffle=True,\n",
    "                                             pin_memory=True,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the EGNN\n",
    "\n",
    "**2.1 EGNN layer that allows inputs of different lenghts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedEGNN(torch.nn.Module):\n",
    "    def __init__(self, node_dim, message_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        edge_input_dim = (node_dim * 2) + 1\n",
    "\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(edge_input_dim, 2*edge_input_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(2*edge_input_dim, message_dim),\n",
    "            torch.nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(node_dim + message_dim, 2*node_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(2*node_dim, node_dim),\n",
    "        )\n",
    "\n",
    "        self.coors_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(message_dim, 2*message_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(2*message_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, node_features, coordinates):                                                        # We pass in a mask that tells us what nodes to consider and which to ignore.\n",
    "        rel_coors = rearrange(coordinates, 'b i d -> b i () d') - rearrange(coordinates, 'b j d -> b () j d')  \n",
    "        rel_dist = (rel_coors ** 2).sum(dim=-1, keepdim=True)                                                  \n",
    "\n",
    "        feats_j = rearrange(node_features, 'b j d -> b () j d')      \n",
    "        feats_i = rearrange(node_features, 'b i d -> b i () d')\n",
    "        feats_i, feats_j = torch.broadcast_tensors(feats_i, feats_j)\n",
    "\n",
    "        edge_input = torch.cat((feats_i, feats_j, rel_dist), dim=-1)\n",
    "\n",
    "        m_ij = self.edge_mlp(edge_input)\n",
    "\n",
    "        coor_weights = self.coors_mlp(m_ij)                                                          # We multiply the predicted weight by the mask (masked residue pairs will have zero weight).\n",
    "        coor_weights = rearrange(coor_weights, 'b i j () -> b i j')\n",
    "\n",
    "        rel_coors_normed = rel_coors / rel_dist.clip(min = 1e-8)    \n",
    "\n",
    "        coors_out = coordinates + torch.einsum('b i j, b i j c -> b i c', coor_weights, rel_coors_normed)  \n",
    "\n",
    "        m_i = m_ij.sum(dim=-2)                                                                      # To average we divide over the length for each batch (length = sum(mask)).\n",
    "\n",
    "        node_mlp_input = torch.cat((node_features, m_i), dim=-1)\n",
    "        node_out = node_features + self.node_mlp(node_mlp_input)                            # We set the update for maked residues to zero. \n",
    "\n",
    "        return node_out, coors_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 EGNN model with 4 layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNNModel(torch.nn.Module):\n",
    "    def __init__(self, node_dim, layers=4, message_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([MaskedEGNN(node_dim, message_dim = message_dim) for _ in range(layers)])   # Initialise as many EGNN layers as needed\n",
    "\n",
    "    def forward(self, node_features, coordinates):\n",
    "\n",
    "        for layer in self.layers:                                                                            \n",
    "            node_features, coordinates = layer(node_features, coordinates)                                      # Update node features and coordinates for each layer in the model\n",
    "        \n",
    "        return node_features, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 5 EGNNs in parallel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoyGen(torch.nn.Module):\n",
    "    def __init__(self, dims_in=41, decoys=5, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks = torch.nn.ModuleList([EGNNModel(node_dim=dims_in, **kwargs) for _ in range(decoys)])\n",
    "        self.decoys = decoys\n",
    "\n",
    "    def forward(self, node_features, coordinates):\n",
    "        geoms = torch.zeros((self.decoys, *coordinates.shape[1:]), device=coordinates.device)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            geoms[i] = block(node_features, coordinates)[1] # only save geoms\n",
    "\n",
    "        return geoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss functions\n",
    "def rmsd(prediction, truth):\n",
    "    dists = (prediction - truth).pow(2).sum(-1)\n",
    "    return torch.sqrt(dists.mean(-1)).mean()\n",
    "\n",
    "def rmsds(preds, true):\n",
    "    return  torch.sort((preds - true).pow(2).sum(-1).mean(-1).pow(1/2))[0]\n",
    "\n",
    "def length_penalty(pred):\n",
    "    return ((((pred[:,1:]-pred[:,:-1])**2).sum(-1).pow(1/2) - 3.802).pow(2)).mean()\n",
    "\n",
    "def different_penalty(pred):\n",
    "    return -(rearrange(pred, \"i n d -> i () n d\") - rearrange(pred, \"j n d -> () j n d\")).pow(2).mean()\n",
    "\n",
    "def dist_check(pred, amino):\n",
    "    err = 0\n",
    "    for i in range(6):\n",
    "        CDR = rearrange(pred[:,amino[0,:,30+i]==1.0], \"d (r a) p -> d a r p\", a = 4)\n",
    "        # CA-CA\n",
    "        err += (((CDR[:,0,1:] - CDR[:,0,:-1]).pow(2).sum(-1).pow(1/2) - 3.82).abs() - 0.12).clamp(0).mean()\n",
    "        # CA-N\n",
    "        err += (((CDR[:,0] - CDR[:,1]).pow(2).sum(-1).pow(1/2) - 1.47).abs() - 0.01).clamp(0).mean()\n",
    "        # CA-C\n",
    "        err += (((CDR[:,0] - CDR[:,2]).pow(2).sum(-1).pow(1/2) - 1.53).abs() - 0.01).clamp(0).mean()\n",
    "        # C-N\n",
    "        err += (((CDR[:,2,:-1] - CDR[:,1,1:]).pow(2).sum(-1).pow(1/2) - 1.34).abs() - 0.01).clamp(0).mean()\n",
    "        # CA-CB\n",
    "        CDR2 = rearrange(pred[:,(amino[0,:,30+i]==1.0) & (amino[0,:,5] != 1.0)], \"d (r a) p -> d a r p\", a = 4)\n",
    "        err += (((CDR2[:,0] - CDR2[:,-1]).pow(2).sum(-1).pow(1/2) - 1.54).abs() - 0.01).clamp(0).mean()\n",
    "\n",
    "    return err\n",
    "\n",
    "def atom_dist(geom):\n",
    "    return ((geom[:,None] - geom[:,:,None]).mean(0).pow(2).sum(-1) + 1e-8).pow(1/2) \n",
    "\n",
    "def atom_dist_penal(geom, pred):\n",
    "    true_ds = atom_dist(geom)\n",
    "    pred_ds = atom_dist(pred)\n",
    "    mask = true_ds < 4.0\n",
    "    return (true_ds-pred_ds)[mask].pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for one epoch\n",
    "def run_epoch(model, optim, train_dataloader, test_dataloader, grad_clip=10.0):\n",
    "    epoch_train_losses = []\n",
    "    model.train()                                                      # Set the model to train mode (Should't matter here as we don't have dropout, but good practice to keep in)\n",
    "\n",
    "    for i,data in enumerate(train_dataloader):                         # For each batch of data in the dataset\n",
    "        coordinates, geomouts, node_features = data['geomins'].float(), data['geomouts'].float(), data['encodings'].float()\n",
    "\n",
    "        pred = model(node_features, coordinates)\n",
    "        optim.zero_grad()                                              # Delete old gradients\n",
    "\n",
    "        loss = rmsd(geomouts, pred)+ 5*(20*atom_dist_penal(geomouts, pred) + dist_check(pred.mean(0, keepdim = True), node_features))\n",
    "        epoch_train_losses.append(loss.item())                         # Store value of loss function for training set\n",
    "\n",
    "        loss.backward()                                                # Calculate loss gradients (pytorch handles this in the background)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # Optional: Clip the norm of the gradient (It stops the optimiser from doing very large updates at once)\n",
    "        optim.step()                                                   # Update model weights\n",
    "\n",
    "    with torch.no_grad():                                              # Calculate loss funtion for validation set\n",
    "        model.eval()                                                   # Set the model to eval mode\n",
    "        epoch_test_loss = np.mean([rmsd(geomouts, pred) + 5*(20*atom_dist_penal(geomouts, pred)\n",
    "                                   + dist_check(pred.mean(0, keepdim = True), data['encodings'])).item() for data in test_dataloader])\n",
    "    \n",
    "    return np.mean(epoch_train_losses), epoch_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "# initialise model\n",
    "model = DecoyGen().to(device = device).float()\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# set optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for 5000 epoch, but stop training if the loss hasn't improved for 150 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train |  Val \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8x/40q8fgwd2wg9ptnzw_b8cl480000gn/T/ipykernel_40069/2876755586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run one epoch and get train and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m                                                    \u001b[0;31m# Store train and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8x/40q8fgwd2wg9ptnzw_b8cl480000gn/T/ipykernel_40069/3397851552.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, optim, train_dataloader, test_dataloader, grad_clip)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeomouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geomins'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geomouts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encodings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                              \u001b[0;31m# Delete old gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8x/40q8fgwd2wg9ptnzw_b8cl480000gn/T/ipykernel_40069/207810879.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_features, coordinates)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mgeoms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# only save geoms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeoms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8x/40q8fgwd2wg9ptnzw_b8cl480000gn/T/ipykernel_40069/1886014286.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_features, coordinates)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;31m# Update node features and coordinates for each layer in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8x/40q8fgwd2wg9ptnzw_b8cl480000gn/T/ipykernel_40069/1866546038.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_features, coordinates)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mm_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcoor_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoors_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_ij\u001b[0m\u001b[0;34m)\u001b[0m                                                          \u001b[0;31m# We multiply the predicted weight by the mask (masked residue pairs will have zero weight).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mcoor_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoor_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b i j () -> b i j'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/auto-db-pipeline/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2032\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\" Train |  Val \")\n",
    "patience = 150\n",
    "for epoch in range(5000):\n",
    "    train_loss, test_loss = run_epoch(model, optimiser, train_dataloader, test_dataloader)  # Run one epoch and get train and validation loss\n",
    "    \n",
    "    train_losses.append(train_loss)                                                    # Store train and validation loss\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if np.min(test_losses) == test_loss:                                                 # If it is the best model on the validation set, save it\n",
    "        torch.save(model.state_dict(), \"best_model\")                                   # This is how you save models in pytorch\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "    elif epochs_without_improvement < patience:                                        # If the model hasn't improved this epoch store that\n",
    "        epochs_without_improvement += 1\n",
    "    else:                                                                              # If the model hasn't improved in 'patience' epochs stop the training.\n",
    "        break\n",
    "\n",
    "    if train_loss > 1.5*np.min(train_losses):                                          # EGNNs are quite unstable, this reverts the model to a previous state if an epoch blows up\n",
    "        model.load_state_dict(torch.load(\"previous_weights\", map_location=torch.device(device)))\n",
    "        optimiser.load_state_dict(torch.load(\"previous_optim\", map_location=torch.device(device)))\n",
    "    if train_loss == np.min(train_losses):\n",
    "        torch.save(model.state_dict(), \"previous_weights\")        \n",
    "        torch.save(optimiser.state_dict(), \"previous_optim\")  \n",
    "\n",
    "\n",
    "    print(\"{:6.2f} | {:6.2f}\".format(train_loss, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b68f9eedec4825490a01d6907fa8139c5284fcb4395b458f0f41fe7d228fceae"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('ab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
